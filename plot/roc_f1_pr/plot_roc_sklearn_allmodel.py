import torch
import os
import numpy as np
from torchvision.datasets import ImageFolder
from utils.transform import get_transform_for_test
from scipy import interp
import matplotlib.pyplot as plt
from itertools import cycle
from sklearn.metrics import roc_curve, auc

from senet.se_resnet import FineTuneSEResnet50
from pytorch_models.finetune import FineTuneVGG16, FineTuneInceptionv3, FineTuneResnet50
from pytorch_models.DFL import DFL_VGG16
from pytorch_models.BCNN import BCNN_resnet50

os.environ['CUDA_VISIBLE_DEVICES'] = "0"

# Áßç
data_root = r"C:\Users\admin\Desktop\fsdownload\set113\test100\species"
num_class = 113
save_path = "test100_roc_all_models_species.jpg"

gpu = "cuda:0"
font = {'family': "Arial"}


x_points = [0.009811320754716982, 0.0098, 0.007735849056603773, 0.003953488372093023, 0.005957446808510639,
            0.010192307692307691, 0.0106, 0.008636363636363636, 0.01183673469387755, 0.01, 0.006666666666666667, 0.0132,
            0.012857142857142857, 0.014130434782608696, 0.013095238095238096, 0.015952380952380954,
            0.018292682926829267, 0.018780487804878048, 0.019285714285714285, 0.018571428571428572, 0.01951219512195122]
y_points = [0.48, 0.51, 0.59, 0.83, 0.72, 0.47, 0.47, 0.62, 0.42, 0.4, 0.64, 0.34, 0.46, 0.35, 0.45, 0.33, 0.25, 0.23,
            0.19, 0.22, 0.2]
expert_type_list = ["s", "s", "s", "s", "s", "s", "j", "s", "j", "j", "s", "j", "j", "j", "o", "o", "o", "o", "o", "o", "o"]



model_fpr_list = [
    [0.0, 0.0, 0.0, 8.928571428571429e-05, 8.928571428571429e-05, 0.00017857142857142857, 0.00017857142857142857, 0.00026785714285714287, 0.00026785714285714287, 0.00035714285714285714, 0.00035714285714285714, 0.0004464285714285714, 0.0004464285714285714, 0.0005357142857142857, 0.0005357142857142857, 0.000625, 0.000625, 0.0007142857142857143, 0.0007142857142857143, 0.0008035714285714286, 0.0008035714285714286, 0.0009821428571428572, 0.0009821428571428572, 0.00125, 0.00125, 0.0016071428571428571, 0.0016071428571428571, 0.0033928571428571428, 0.0033928571428571428, 0.0038392857142857144, 0.0038392857142857144, 0.0040178571428571425, 0.0040178571428571425, 0.0041964285714285714, 0.0041964285714285714, 0.004285714285714286, 0.004285714285714286, 0.004464285714285714, 0.004464285714285714, 0.005892857142857143, 0.005892857142857143, 0.005982142857142857, 0.005982142857142857, 0.008392857142857143, 0.008392857142857143, 0.008839285714285714, 0.008839285714285714, 0.009285714285714286, 0.009285714285714286, 0.01, 0.01, 0.011517857142857142, 0.011517857142857142, 0.014285714285714285, 0.014285714285714285, 0.014910714285714286, 0.014910714285714286, 0.015, 0.015, 0.018482142857142857, 0.018482142857142857, 0.018571428571428572, 0.018571428571428572, 0.021875, 0.021875, 0.0225, 0.0225, 0.023392857142857142, 0.023392857142857142, 0.026875, 0.026875, 0.029285714285714286, 0.029285714285714286, 0.029375, 0.029375, 0.04383928571428571, 0.04383928571428571, 0.052767857142857144, 0.052767857142857144, 0.05580357142857143, 0.05580357142857143, 0.062142857142857146, 0.062142857142857146, 0.06803571428571428, 0.06803571428571428, 0.07357142857142857, 0.07357142857142857, 0.11142857142857143, 0.11142857142857143, 1.0],
    [0.0, 0.0, 0.0, 8.928571428571429e-05, 8.928571428571429e-05, 0.00017857142857142857, 0.00017857142857142857, 0.00026785714285714287, 0.00026785714285714287, 0.00035714285714285714, 0.00035714285714285714, 0.0004464285714285714, 0.0004464285714285714, 0.0005357142857142857, 0.0005357142857142857, 0.000625, 0.000625, 0.0007142857142857143, 0.0007142857142857143, 0.0008035714285714286, 0.0008035714285714286, 0.0009821428571428572, 0.0009821428571428572, 0.00125, 0.00125, 0.0016071428571428571, 0.0016071428571428571, 0.0016964285714285714, 0.0016964285714285714, 0.002589285714285714, 0.002589285714285714, 0.0029464285714285716, 0.0029464285714285716, 0.0030357142857142857, 0.0030357142857142857, 0.003660714285714286, 0.003660714285714286, 0.004107142857142857, 0.004107142857142857, 0.004285714285714286, 0.004285714285714286, 0.00830357142857143, 0.00830357142857143, 0.009642857142857142, 0.009642857142857142, 0.011785714285714287, 0.011785714285714287, 0.014285714285714285, 0.014285714285714285, 0.015357142857142857, 0.015357142857142857, 0.024464285714285713, 0.024464285714285713, 0.02544642857142857, 0.02544642857142857, 0.025892857142857145, 0.025892857142857145, 0.03017857142857143, 0.03017857142857143, 0.041875, 0.041875, 0.045535714285714284, 0.045535714285714284, 0.047589285714285716, 0.047589285714285716, 0.04776785714285714, 0.04776785714285714, 0.10330357142857143, 0.10330357142857143, 0.2330357142857143, 0.2330357142857143, 1.0],
    [0.0, 0.0, 0.0, 0.00026785714285714287, 0.00026785714285714287, 0.00035714285714285714, 0.00035714285714285714, 0.0005357142857142857, 0.0005357142857142857, 0.000625, 0.000625, 0.0007142857142857143, 0.0007142857142857143, 0.0008928571428571428, 0.0008928571428571428, 0.0009821428571428572, 0.0009821428571428572, 0.0011607142857142858, 0.0011607142857142858, 0.00125, 0.00125, 0.0014285714285714286, 0.0014285714285714286, 0.0016071428571428571, 0.0016071428571428571, 0.0016964285714285714, 0.0016964285714285714, 0.0017857142857142857, 0.0017857142857142857, 0.0019642857142857144, 0.0019642857142857144, 0.002232142857142857, 0.002232142857142857, 0.0027678571428571427, 0.0027678571428571427, 0.0033035714285714287, 0.0033035714285714287, 0.0033928571428571428, 0.0033928571428571428, 0.0040178571428571425, 0.0040178571428571425, 0.0041964285714285714, 0.0041964285714285714, 0.004464285714285714, 0.004464285714285714, 0.004642857142857143, 0.004642857142857143, 0.005267857142857143, 0.005267857142857143, 0.005446428571428572, 0.005446428571428572, 0.005714285714285714, 0.005714285714285714, 0.005892857142857143, 0.005892857142857143, 0.009553571428571429, 0.009553571428571429, 0.012678571428571428, 0.012678571428571428, 0.01669642857142857, 0.01669642857142857, 0.018839285714285715, 0.018839285714285715, 0.025089285714285713, 0.025089285714285713, 0.03741071428571428, 0.03741071428571428, 0.040357142857142855, 0.040357142857142855, 0.040982142857142856, 0.040982142857142856, 0.05803571428571429, 0.05803571428571429, 0.07794642857142857, 0.07794642857142857, 0.15517857142857142, 0.15517857142857142, 0.2238392857142857, 0.2238392857142857, 1.0],
    [0.0, 0.0, 0.0, 8.928571428571429e-05, 8.928571428571429e-05, 0.00017857142857142857, 0.00017857142857142857, 0.00026785714285714287, 0.00026785714285714287, 0.00035714285714285714, 0.00035714285714285714, 0.000625, 0.000625, 0.0007142857142857143, 0.0007142857142857143, 0.0008035714285714286, 0.0008035714285714286, 0.0009821428571428572, 0.0009821428571428572, 0.0010714285714285715, 0.0010714285714285715, 0.0014285714285714286, 0.0014285714285714286, 0.0015178571428571428, 0.0015178571428571428, 0.0016071428571428571, 0.0016071428571428571, 0.0016964285714285714, 0.0016964285714285714, 0.001875, 0.001875, 0.0019642857142857144, 0.0019642857142857144, 0.0020535714285714285, 0.0020535714285714285, 0.0027678571428571427, 0.0027678571428571427, 0.0033035714285714287, 0.0033035714285714287, 0.0035714285714285713, 0.0035714285714285713, 0.0040178571428571425, 0.0040178571428571425, 0.004464285714285714, 0.004464285714285714, 0.00625, 0.00625, 0.0064285714285714285, 0.0064285714285714285, 0.008035714285714285, 0.008035714285714285, 0.008125, 0.008125, 0.010267857142857143, 0.010267857142857143, 0.010535714285714285, 0.010535714285714285, 0.012410714285714285, 0.012410714285714285, 0.014375, 0.014375, 0.017857142857142856, 0.017857142857142856, 0.018839285714285715, 0.018839285714285715, 0.02044642857142857, 0.02044642857142857, 0.03410714285714286, 0.03410714285714286, 0.07919642857142857, 0.07919642857142857, 0.08205357142857143, 0.08205357142857143, 0.08589285714285715, 0.08589285714285715, 0.12392857142857143, 0.12392857142857143, 0.1332142857142857, 0.1332142857142857, 0.13455357142857144, 0.13455357142857144, 0.17392857142857143, 0.17392857142857143, 0.17660714285714285, 0.17660714285714285, 0.27589285714285716, 0.27589285714285716, 0.3482142857142857, 0.3482142857142857, 0.4376785714285714, 0.43785714285714283, 0.4450892857142857, 0.44526785714285716, 0.6183035714285714, 0.6183035714285714, 0.7734821428571429, 0.7736607142857143, 1.0],
    [0.0, 8.928571428571429e-05, 8.928571428571429e-05, 8.928571428571429e-05, 8.928571428571429e-05, 8.928571428571429e-05, 0.00017857142857142857, 0.00017857142857142857, 0.00026785714285714287, 0.00035714285714285714, 0.00035714285714285714, 0.0004464285714285714, 0.0004464285714285714, 0.0005357142857142857, 0.0005357142857142857, 0.0007142857142857143, 0.0007142857142857143, 0.0009821428571428572, 0.0009821428571428572, 0.0010714285714285715, 0.0010714285714285715, 0.0011607142857142858, 0.0011607142857142858, 0.00125, 0.00125, 0.0013392857142857143, 0.0013392857142857143, 0.0014285714285714286, 0.0014285714285714286, 0.001875, 0.001875, 0.0020535714285714285, 0.0020535714285714285, 0.0030357142857142857, 0.0030357142857142857, 0.00375, 0.00375, 0.006339285714285714, 0.006339285714285714, 0.009107142857142857, 0.009107142857142857, 0.015357142857142857, 0.015357142857142857, 0.019107142857142857, 0.019107142857142857, 0.02982142857142857, 0.02982142857142857, 0.03410714285714286, 0.03410714285714286, 0.04044642857142857, 0.04044642857142857, 0.04366071428571429, 0.04366071428571429, 0.06482142857142857, 0.06482142857142857, 0.06553571428571428, 0.06571428571428571, 0.07008928571428572, 0.07008928571428572, 0.07625, 0.07625, 0.07991071428571428, 0.07991071428571428, 0.08223214285714285, 0.08223214285714285, 0.09553571428571428, 0.09553571428571428, 0.10267857142857142, 0.10267857142857142, 0.17473214285714286, 0.17473214285714286, 0.3444642857142857, 0.3444642857142857, 0.37285714285714283, 0.3730357142857143, 0.4458035714285714, 0.4458035714285714, 1.0],
    [0.0, 0.0, 0.0, 8.928571428571429e-05, 8.928571428571429e-05, 0.00017857142857142857, 0.00017857142857142857, 0.00026785714285714287, 0.00026785714285714287, 0.00035714285714285714, 0.00035714285714285714, 0.0007142857142857143, 0.0007142857142857143, 0.0008035714285714286, 0.0008035714285714286, 0.0008928571428571428, 0.0008928571428571428, 0.0011607142857142858, 0.0011607142857142858, 0.0014285714285714286, 0.0014285714285714286, 0.0015178571428571428, 0.0015178571428571428, 0.0016964285714285714, 0.0016964285714285714, 0.0024107142857142856, 0.0024107142857142856, 0.0026785714285714286, 0.0026785714285714286, 0.0027678571428571427, 0.0027678571428571427, 0.0029464285714285716, 0.0029464285714285716, 0.0033035714285714287, 0.0033035714285714287, 0.004107142857142857, 0.004107142857142857, 0.004910714285714286, 0.004910714285714286, 0.005, 0.005, 0.006071428571428571, 0.006071428571428571, 0.008482142857142856, 0.008482142857142856, 0.008839285714285714, 0.008839285714285714, 0.00919642857142857, 0.00919642857142857, 0.013392857142857142, 0.013392857142857142, 0.014017857142857143, 0.014017857142857143, 0.017142857142857144, 0.017142857142857144, 0.02142857142857143, 0.02142857142857143, 0.05830357142857143, 0.05830357142857143, 0.05901785714285714, 0.05901785714285714, 0.15357142857142858, 0.15357142857142858, 0.22223214285714285, 0.22223214285714285, 1.0],

    [0.0, 0.0, 0.0, 8.928571428571429e-05, 8.928571428571429e-05, 0.00017857142857142857, 0.00017857142857142857, 0.00026785714285714287, 0.00026785714285714287, 0.00035714285714285714, 0.00035714285714285714, 0.0004464285714285714, 0.0004464285714285714, 0.0005357142857142857, 0.0005357142857142857, 0.000625, 0.000625, 0.0007142857142857143, 0.0007142857142857143, 0.0008035714285714286, 0.0008035714285714286, 0.0008928571428571428, 0.0008928571428571428, 0.0009821428571428572, 0.0009821428571428572, 0.0010714285714285715, 0.0010714285714285715, 0.00125, 0.00125, 0.0013392857142857143, 0.0013392857142857143, 0.0015178571428571428, 0.0015178571428571428, 0.001875, 0.001875, 0.0020535714285714285, 0.0020535714285714285, 0.002232142857142857, 0.002232142857142857, 0.0024107142857142856, 0.0024107142857142856, 0.0025, 0.0025, 0.0030357142857142857, 0.0030357142857142857, 0.004910714285714286, 0.004910714285714286, 0.007410714285714286, 0.007410714285714286, 0.007946428571428571, 0.007946428571428571, 0.008214285714285714, 0.008214285714285714, 0.008482142857142856, 0.008482142857142856, 0.010535714285714285, 0.010535714285714285, 0.011339285714285715, 0.011339285714285715, 0.014107142857142856, 0.014107142857142856, 0.019196428571428573, 0.019196428571428573, 0.02142857142857143, 0.02142857142857143, 0.023125, 0.023125, 0.025357142857142856, 0.025357142857142856, 0.02651785714285714, 0.02651785714285714, 0.0275, 0.0275, 0.030714285714285715, 0.030714285714285715, 0.03491071428571429, 0.03491071428571429, 0.03875, 0.03875, 0.04151785714285714, 0.04151785714285714, 0.06767857142857144, 0.06767857142857144, 0.09133928571428572, 0.09133928571428572, 0.29642857142857143, 0.29642857142857143, 0.7607142857142857, 0.7608928571428571, 1.0],
    [0.0, 0.0, 0.0, 8.928571428571429e-05, 8.928571428571429e-05, 0.00017857142857142857, 0.00017857142857142857, 0.00026785714285714287, 0.00026785714285714287, 0.0005357142857142857, 0.0005357142857142857, 0.0007142857142857143, 0.0007142857142857143, 0.0008035714285714286, 0.0008035714285714286, 0.0008928571428571428, 0.0008928571428571428, 0.0016964285714285714, 0.0016964285714285714, 0.001875, 0.001875, 0.0024107142857142856, 0.0024107142857142856, 0.002589285714285714, 0.002589285714285714, 0.0032142857142857142, 0.0032142857142857142, 0.004285714285714286, 0.004285714285714286, 0.007142857142857143, 0.007142857142857143, 0.0075, 0.0075, 0.009464285714285715, 0.009464285714285715, 0.009910714285714285, 0.009910714285714285, 0.014553571428571428, 0.014553571428571428, 0.015625, 0.015625, 0.016339285714285716, 0.016339285714285716, 0.016428571428571428, 0.016428571428571428, 0.019285714285714285, 0.019285714285714285, 0.020625, 0.020625, 0.025267857142857144, 0.025267857142857144, 0.028214285714285713, 0.028214285714285713, 0.03616071428571429, 0.03616071428571429, 0.04133928571428572, 0.04133928571428572, 0.053839285714285715, 0.053839285714285715, 0.07758928571428571, 0.07758928571428571, 0.08169642857142857, 0.08169642857142857, 0.08258928571428571, 0.08258928571428571, 0.21410714285714286, 0.21410714285714286, 0.23401785714285714, 0.23401785714285714, 0.3655357142857143, 0.3655357142857143, 0.5565178571428572, 0.5565178571428572, 1.0],
    [0.0, 0.0, 0.0, 8.928571428571429e-05, 8.928571428571429e-05, 0.00017857142857142857, 0.00017857142857142857, 0.00035714285714285714, 0.00035714285714285714, 0.0004464285714285714, 0.0004464285714285714, 0.0005357142857142857, 0.0005357142857142857, 0.000625, 0.000625, 0.0008928571428571428, 0.0008928571428571428, 0.0009821428571428572, 0.0009821428571428572, 0.0010714285714285715, 0.0010714285714285715, 0.0011607142857142858, 0.0011607142857142858, 0.00125, 0.00125, 0.0025, 0.0025, 0.007142857142857143, 0.007142857142857143, 0.007321428571428572, 0.007321428571428572, 0.008035714285714285, 0.008035714285714285, 0.008660714285714285, 0.008660714285714285, 0.010267857142857143, 0.010267857142857143, 0.013482142857142857, 0.013482142857142857, 0.014642857142857143, 0.014642857142857143, 0.020267857142857143, 0.020267857142857143, 0.023125, 0.023125, 0.033482142857142856, 0.033482142857142856, 0.03482142857142857, 0.03482142857142857, 0.03625, 0.03625, 0.036517857142857144, 0.036517857142857144, 0.049553571428571426, 0.049553571428571426, 0.09964285714285714, 0.09964285714285714, 0.10732142857142857, 0.10732142857142857, 0.14008928571428572, 0.14008928571428572, 0.9197321428571429, 0.9199107142857142, 1.0],
    [0.0, 0.0, 0.0, 8.928571428571429e-05, 8.928571428571429e-05, 0.00035714285714285714, 0.00035714285714285714, 0.000625, 0.000625, 0.0007142857142857143, 0.0007142857142857143, 0.0008035714285714286, 0.0008035714285714286, 0.0008928571428571428, 0.0008928571428571428, 0.0009821428571428572, 0.0009821428571428572, 0.0011607142857142858, 0.0011607142857142858, 0.00125, 0.00125, 0.0013392857142857143, 0.0013392857142857143, 0.0015178571428571428, 0.0015178571428571428, 0.001875, 0.001875, 0.0019642857142857144, 0.0019642857142857144, 0.002142857142857143, 0.002142857142857143, 0.0023214285714285715, 0.0023214285714285715, 0.0024107142857142856, 0.0024107142857142856, 0.0025, 0.0025, 0.0033928571428571428, 0.0033928571428571428, 0.0035714285714285713, 0.0035714285714285713, 0.004821428571428571, 0.004821428571428571, 0.004910714285714286, 0.004910714285714286, 0.005535714285714285, 0.005535714285714285, 0.005625, 0.005625, 0.005892857142857143, 0.005892857142857143, 0.007053571428571428, 0.007053571428571428, 0.007946428571428571, 0.007946428571428571, 0.00919642857142857, 0.00919642857142857, 0.009821428571428571, 0.009821428571428571, 0.02035714285714286, 0.02035714285714286, 0.021517857142857144, 0.021517857142857144, 0.023035714285714284, 0.023035714285714284, 0.023303571428571427, 0.023303571428571427, 0.028392857142857143, 0.028392857142857143, 0.02919642857142857, 0.02919642857142857, 0.040803571428571425, 0.040803571428571425, 0.042053571428571426, 0.042053571428571426, 0.048928571428571425, 0.048928571428571425, 0.057053571428571426, 0.057053571428571426, 0.07366071428571429, 0.07366071428571429, 0.07758928571428571, 0.07758928571428571, 0.1025, 0.1025, 0.11821428571428572, 0.11821428571428572, 0.125, 0.125, 0.13651785714285714, 0.13651785714285714, 0.2017857142857143, 0.2017857142857143, 0.21026785714285715, 0.21026785714285715, 0.2319642857142857, 0.2319642857142857, 0.326875, 0.326875, 0.3817857142857143, 0.3817857142857143, 0.5913392857142857, 0.5915178571428571, 0.6890178571428571, 0.6890178571428571, 0.6934821428571428, 0.6934821428571428, 1.0],
][:-1]
model_tpr_list = [
    [0.0, 0.03, 0.37, 0.37, 0.4, 0.4, 0.41, 0.41, 0.42, 0.42, 0.45, 0.45, 0.49, 0.49, 0.58, 0.58, 0.61, 0.61, 0.62, 0.62, 0.63, 0.63, 0.64, 0.64, 0.66, 0.66, 0.67, 0.67, 0.68, 0.68, 0.69, 0.69, 0.7, 0.7, 0.71, 0.71, 0.72, 0.72, 0.73, 0.73, 0.74, 0.74, 0.75, 0.75, 0.76, 0.76, 0.77, 0.77, 0.78, 0.78, 0.79, 0.79, 0.8, 0.8, 0.81, 0.81, 0.82, 0.82, 0.83, 0.83, 0.85, 0.85, 0.86, 0.86, 0.87, 0.87, 0.88, 0.88, 0.89, 0.89, 0.9, 0.9, 0.91, 0.91, 0.93, 0.93, 0.94, 0.94, 0.95, 0.95, 0.96, 0.96, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 1.0, 1.0],
    [0.0, 0.01, 0.19, 0.19, 0.35, 0.35, 0.51, 0.51, 0.54, 0.54, 0.6, 0.6, 0.63, 0.63, 0.64, 0.64, 0.67, 0.67, 0.69, 0.69, 0.74, 0.74, 0.75, 0.75, 0.77, 0.77, 0.78, 0.78, 0.79, 0.79, 0.8, 0.8, 0.81, 0.81, 0.82, 0.82, 0.83, 0.83, 0.84, 0.84, 0.85, 0.85, 0.86, 0.86, 0.87, 0.87, 0.88, 0.88, 0.89, 0.89, 0.9, 0.9, 0.91, 0.91, 0.92, 0.92, 0.93, 0.93, 0.94, 0.94, 0.95, 0.95, 0.96, 0.96, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 1.0, 1.0],
    [0.0, 0.01, 0.38, 0.38, 0.39, 0.39, 0.53, 0.53, 0.54, 0.54, 0.55, 0.55, 0.57, 0.57, 0.61, 0.61, 0.62, 0.62, 0.64, 0.64, 0.66, 0.66, 0.67, 0.67, 0.68, 0.68, 0.71, 0.71, 0.72, 0.72, 0.74, 0.74, 0.75, 0.75, 0.76, 0.76, 0.78, 0.78, 0.79, 0.79, 0.81, 0.81, 0.82, 0.82, 0.83, 0.83, 0.84, 0.84, 0.85, 0.85, 0.86, 0.86, 0.87, 0.87, 0.88, 0.88, 0.89, 0.89, 0.9, 0.9, 0.91, 0.91, 0.92, 0.92, 0.93, 0.93, 0.94, 0.94, 0.95, 0.95, 0.96, 0.96, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 1.0, 1.0],
    [0.0, 0.01, 0.25, 0.25, 0.27, 0.27, 0.3, 0.3, 0.37, 0.37, 0.45, 0.45, 0.5, 0.5, 0.51, 0.51, 0.52, 0.52, 0.55, 0.55, 0.57, 0.57, 0.59, 0.59, 0.6, 0.6, 0.62, 0.62, 0.66, 0.66, 0.67, 0.67, 0.68, 0.68, 0.69, 0.69, 0.7, 0.7, 0.71, 0.71, 0.72, 0.72, 0.73, 0.73, 0.74, 0.74, 0.76, 0.76, 0.77, 0.77, 0.78, 0.78, 0.8, 0.8, 0.81, 0.81, 0.82, 0.82, 0.83, 0.83, 0.84, 0.84, 0.85, 0.85, 0.86, 0.86, 0.87, 0.87, 0.88, 0.88, 0.89, 0.89, 0.9, 0.9, 0.91, 0.91, 0.92, 0.92, 0.93, 0.93, 0.95, 0.95, 0.96, 0.96, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0],
    [0.0, 0.3, 0.33, 0.34, 0.36, 0.38, 0.38, 0.43, 0.44, 0.44, 0.48, 0.48, 0.5, 0.5, 0.54, 0.54, 0.59, 0.59, 0.61, 0.61, 0.62, 0.62, 0.63, 0.63, 0.68, 0.68, 0.7, 0.7, 0.71, 0.71, 0.73, 0.73, 0.78, 0.78, 0.8, 0.8, 0.82, 0.82, 0.83, 0.83, 0.84, 0.84, 0.85, 0.85, 0.86, 0.86, 0.87, 0.87, 0.88, 0.88, 0.89, 0.89, 0.9, 0.9, 0.91, 0.91, 0.91, 0.91, 0.92, 0.92, 0.93, 0.93, 0.94, 0.94, 0.95, 0.95, 0.96, 0.96, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 0.99, 0.99, 1.0, 1.0],
    [0.0, 0.01, 0.4, 0.4, 0.51, 0.51, 0.59, 0.59, 0.61, 0.61, 0.65, 0.65, 0.66, 0.66, 0.7, 0.7, 0.73, 0.73, 0.76, 0.76, 0.77, 0.77, 0.78, 0.78, 0.79, 0.79, 0.8, 0.8, 0.81, 0.81, 0.82, 0.82, 0.83, 0.83, 0.84, 0.84, 0.85, 0.85, 0.86, 0.86, 0.87, 0.87, 0.88, 0.88, 0.89, 0.89, 0.9, 0.9, 0.92, 0.92, 0.93, 0.93, 0.94, 0.94, 0.95, 0.95, 0.96, 0.96, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 1.0, 1.0],

    [0.0, 0.01, 0.16, 0.16, 0.2, 0.2, 0.21, 0.21, 0.28, 0.28, 0.37, 0.37, 0.43, 0.43, 0.45, 0.45, 0.53, 0.53, 0.54, 0.54, 0.55, 0.55, 0.56, 0.56, 0.59, 0.59, 0.62, 0.62, 0.64, 0.64, 0.66, 0.66, 0.67, 0.67, 0.7, 0.7, 0.73, 0.73, 0.74, 0.74, 0.75, 0.75, 0.76, 0.76, 0.79, 0.79, 0.8, 0.8, 0.81, 0.81, 0.82, 0.82, 0.83, 0.83, 0.84, 0.84, 0.85, 0.85, 0.86, 0.86, 0.87, 0.87, 0.88, 0.88, 0.89, 0.89, 0.9, 0.9, 0.91, 0.91, 0.92, 0.92, 0.93, 0.93, 0.94, 0.94, 0.95, 0.95, 0.96, 0.96, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0],

    [0.0, 0.01, 0.06, 0.06, 0.1, 0.1, 0.49, 0.49, 0.53, 0.53, 0.54, 0.54, 0.55, 0.55, 0.63, 0.63, 0.68, 0.68, 0.71, 0.71, 0.73, 0.73, 0.74, 0.74, 0.75, 0.75, 0.76, 0.76, 0.78, 0.78, 0.79, 0.79, 0.8, 0.8, 0.81, 0.81, 0.82, 0.82, 0.83, 0.83, 0.84, 0.84, 0.85, 0.85, 0.86, 0.86, 0.87, 0.87, 0.88, 0.88, 0.89, 0.89, 0.9, 0.9, 0.91, 0.91, 0.92, 0.92, 0.93, 0.93, 0.94, 0.94, 0.95, 0.95, 0.96, 0.96, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 1.0, 1.0],
    [0.0, 0.01, 0.39, 0.39, 0.44, 0.44, 0.49, 0.49, 0.5, 0.5, 0.61, 0.61, 0.66, 0.66, 0.67, 0.67, 0.7, 0.7, 0.71, 0.71, 0.73, 0.73, 0.75, 0.75, 0.8, 0.8, 0.81, 0.81, 0.83, 0.83, 0.84, 0.84, 0.85, 0.85, 0.86, 0.86, 0.87, 0.87, 0.88, 0.88, 0.89, 0.89, 0.9, 0.9, 0.91, 0.91, 0.92, 0.92, 0.93, 0.93, 0.94, 0.94, 0.96, 0.96, 0.97, 0.97, 0.98, 0.98, 0.99, 0.99, 1.0, 1.0, 1.0, 1.0],
    [0.0, 0.01, 0.21, 0.21, 0.23, 0.23, 0.26, 0.26, 0.27, 0.27, 0.33, 0.33, 0.35, 0.35, 0.46, 0.46, 0.48, 0.48, 0.49, 0.49, 0.56, 0.56, 0.57, 0.57, 0.58, 0.58, 0.59, 0.59, 0.6, 0.6, 0.61, 0.61, 0.62, 0.62, 0.64, 0.64, 0.65, 0.65, 0.67, 0.67, 0.68, 0.68, 0.69, 0.69, 0.7, 0.7, 0.71, 0.71, 0.72, 0.72, 0.73, 0.73, 0.74, 0.74, 0.75, 0.75, 0.76, 0.76, 0.77, 0.77, 0.78, 0.78, 0.79, 0.79, 0.8, 0.8, 0.81, 0.81, 0.82, 0.82, 0.83, 0.83, 0.84, 0.84, 0.85, 0.85, 0.86, 0.86, 0.87, 0.87, 0.88, 0.88, 0.89, 0.89, 0.9, 0.9, 0.91, 0.91, 0.92, 0.92, 0.93, 0.93, 0.94, 0.94, 0.95, 0.95, 0.96, 0.96, 0.97, 0.97, 0.98, 0.98, 0.98, 0.98, 0.99, 0.99, 1.0, 1.0],
][:-1]
model_auc_list = [0.9914901785714285, 0.9927169642857143, 0.9919366071428571, 0.9739803571428572, 0.982121875, 0.99351875, 0.9912491071428572, 0.9802008928571428, 0.9927892857142857, 0.9617758928571428][:-1]
model_name_list = ["VGG16", "Resnet50", "Inception_v3", "B-CNN", "DFL-CNN", "SE-Resnet50", "PC", "NTS-Net", "MC-Loss", "HBP"][:-1]
base_name_list = ["", "", "", "VGG16", "VGG16", "Resnet50", "Resnet50", "Resnet50", "Resnet50", "VGG16"][:-1]

# mean=[0.948078, 0.93855226, 0.9332005], var=[0.14589554, 0.17054074, 0.18254866]
def roc_points(model, model_name, weights_path):
    global fpr_dict, tpr_dict
    # Âä†ËΩΩÊµãËØïÈõÜ
    test_dir = os.path.join(data_root, 'test_images')
    transform_test = get_transform_for_test()
    test_dataset = ImageFolder(test_dir, transform=transform_test)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
    # Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂèÇÊï∞
    checkpoint = torch.load(weights_path)
    model.load_state_dict(checkpoint['state_dict'])
    model.eval()

    score_list = []  # Â≠òÂÇ®È¢ÑÊµãÂæóÂàÜ
    label_list = []  # Â≠òÂÇ®ÁúüÂÆûÊ†áÁ≠æ
    for i, (inputs, labels) in enumerate(test_loader):
        inputs = inputs.cuda()
        labels = labels.cuda()

        outputs = model(inputs)
        prob_tmp = torch.nn.Softmax(dim=1)(outputs)  # (batchsize, nclass)
        score_tmp = prob_tmp  # (batchsize, nclass)

        score_list.extend(score_tmp.detach().cpu().numpy())
        label_list.extend(labels.cpu().numpy())

    score_array = np.array(score_list)
    # Â∞ÜlabelËΩ¨Êç¢ÊàêonehotÂΩ¢Âºè
    label_tensor = torch.tensor(label_list)
    label_tensor = label_tensor.reshape((label_tensor.shape[0], 1))
    label_onehot = torch.zeros(label_tensor.shape[0], num_class)
    print(label_onehot.shape, label_tensor.shape)
    label_onehot.scatter_(dim=1, index=label_tensor, value=1)
    label_onehot = np.array(label_onehot)

    print("score_array:", score_array.shape)  # (batchsize, classnum)
    print("label_onehot:", label_onehot.shape)  # torch.Size([batchsize, classnum])

    # Ë∞ÉÁî®sklearnÂ∫ìÔºåËÆ°ÁÆóÊØè‰∏™Á±ªÂà´ÂØπÂ∫îÁöÑfprÂíåtpr
    fpr_dict = dict()
    tpr_dict = dict()
    roc_auc_dict = dict()
    fpr_dict[model_name] = dict()
    tpr_dict[model_name] = dict()
    roc_auc_dict[model_name] = dict()
    for i in range(num_class):
        fpr_dict[i], tpr_dict[i], _ = roc_curve(label_onehot[:, i], score_array[:, i])
        roc_auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])
    # micro
    fpr_dict[model_name]["micro"], tpr_dict[model_name]["micro"], _ = roc_curve(label_onehot.ravel(), score_array.ravel())
    roc_auc_dict[model_name]["micro"] = auc(fpr_dict[model_name]["micro"], tpr_dict[model_name]["micro"])
    print(len(list(fpr_dict[model_name]["micro"])), list(fpr_dict[model_name]["micro"]))
    print(len(list(tpr_dict[model_name]["micro"])), list(tpr_dict[model_name]["micro"]))
    print(roc_auc_dict[model_name]["micro"])


def plot_function():
    global x_points, y_points
    x_points = [1 - x_points[i] for i in range(len(x_points))]
    global model_fpr_list, model_tpr_list, model_auc_list, model_name_list, expert_type_list
    max_length = 0
    for l in model_fpr_list:
        if len(l) > max_length:
            max_length = len(l)

    model_fpr_avg, model_tpr_avg = [], []
    temp_fpr_list, temp_tpr_list = [], []
    for i in range(max_length):
        for model_fpr, model_tpr in zip(model_fpr_list, model_tpr_list):
            if len(model_fpr) > i:
                temp_fpr_list.append(model_fpr[i])
                temp_tpr_list.append(model_tpr[i])
        model_fpr_avg.append(np.mean(temp_fpr_list))
        model_tpr_avg.append(np.mean(temp_tpr_list))
        temp_fpr_list.clear()
        temp_tpr_list.clear()
    model_auc_avg = np.mean(model_auc_list)

    x_points_academic, y_points_academic, x_points_industry, y_points_industry = [], [], [], []
    for i in range(len(x_points)):
        if expert_type_list[i] in ["j", "s"]:
            x_points_academic.append(x_points[i])
            y_points_academic.append(y_points[i])
        else:
            x_points_industry.append(x_points[i])
            y_points_industry.append(y_points[i])

    # ÁªòÂà∂ÊâÄÊúâÁ±ªÂà´Âπ≥ÂùáÁöÑrocÊõ≤Á∫ø
    plt.figure()
    lw = 2
    plt.rcParams['savefig.dpi'] = 600  # ÂõæÁâáÂÉèÁ¥†
    plt.rcParams['figure.dpi'] = 600  # ÂàÜËæ®Áéá
    plt.grid(linestyle='--')

    x_point_academic_avg = np.mean(x_points_academic)
    y_point_academic_avg = np.mean(y_points_academic)
    x_point_industry_avg = np.mean(x_points_industry)
    y_point_industry_avg = np.mean(y_points_industry)

    # plt.plot([0, 1], [0, 1], 'k--', lw=lw)

    # ‰∏∫ÊØè‰∏™Ê®°ÂûãÁªòÂà∂‰∏ÄÊù°Êõ≤Á∫ø
    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'navy', 'deeppink', "#E18D65", "#88C0A4", "green", "red", "#00AEEF"])
    for i, color in zip(range(len(model_fpr_list)), colors):
        if i > 2 and base_name_list[i] == "Resnet50":
            plt.plot(1 - np.array(model_fpr_list[i]), np.array(model_tpr_list[i]), color=color, lw=1, linestyle='-',
                     label='micro-average ROC of {0}# (area = {1:.3f})'.format(model_name_list[i], model_auc_list[i]))
        elif i > 2:
            plt.plot(1 - np.array(model_fpr_list[i]), np.array(model_tpr_list[i]), color=color, lw=1, linestyle='-',
                     label='micro-average ROC of {0}* (area = {1:.3f})'.format(model_name_list[i], model_auc_list[i]))
        else:
            plt.plot(1 - np.array(model_fpr_list[i]), np.array(model_tpr_list[i]), color=color, lw=1, linestyle='-',
                     label='micro-average ROC of {0} (area = {1:0.3f})'.format(model_name_list[i], model_auc_list[i]))

    # ÁªòÂà∂‰∏ÄÊù°Âπ≥ÂùáÊ®°ÂûãÊõ≤Á∫ø
    # plt.plot(1 - np.array(model_fpr_avg), np.array(model_tpr_avg), color="#E18D65", lw=lw, linestyle='-',
    #          label='micro-average ROC of all models (area = {0:0.2f})'.format(model_auc_avg))
    # plt.plot(x_point_academic_avg, y_point_academic_avg, 'o', color='red', markersize=3)
    # plt.plot(x_point_industry_avg, y_point_industry_avg, '+', color="green", markersize=12)

    # names = [str(i) for i in range(1, 22)]
    # for i in range(len(y_points)):
    #     plt.text(x_points[i] + 0.015, y_points[i] - 0.015, names[i], fontsize=3, fontdict=font)
    # if i != 3 and i != 4:
    # plt.text(x_points[i] + 0.015, y_points[i] - 0.015, names[i], fontsize=8, fontdict=font)
    # else:
    # plt.text(x_points[3] + 0.015, y_points[3] - 0.015, names[3] + ", " + names[4], fontsize=8, fontdict=font)

    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel('False Positive Rate', fontdict=font)
    plt.ylabel('True Positive Rate', fontdict=font)
    plt.xlabel('Specificity', fontdict=font)
    plt.ylabel('Sensitivity', fontdict=font)
    plt.title('Receiver operating characteristic to multi-class', fontdict=font)
    plt.legend(loc="lower left", fontsize=8)
    plt.savefig(save_path)
    plt.show()


if __name__ == '__main__':
    # Âä†ËΩΩÊ®°Âûã
    # model = FineTuneVGG16(num_class=num_class, pretrained=False)
    # model_name = "vgg16_bn"
    # weights_path = r"D:\TJU\GBDB\set113\models\test100\species\VGG16_bn\epoch_0018_top1_70.00000_'checkpoint.pth.tar'"
    # model = FineTuneResnet50(num_class=num_class, pretrained=False)
    # model_name = "resnet50"
    # weights_path = r"D:\TJU\GBDB\set113\models\test100\species\Resnet50\epoch_0032_top1_78.00000_'checkpoint.pth.tar'"
    # model = FineTuneInceptionv3(num_class=num_class, pretrained=False)
    # model_name = "inception_v3"
    # weights_path = r"D:\TJU\GBDB\set113\models\test100\species\Inception_v3\epoch_0022_top1_77.00000_'checkpoint.pth.tar'"
    # model = FineTuneSEResnet50(num_class=num_class)
    # model_name = "seresnet50"
    # weights_path = r"D:\TJU\GBDB\set113\models\test100\species\SE-Resnet50\epoch_0041_top1_81.000_'checkpoint.pth.tar'"
    # model = DFL_VGG16(k=10, nclass=num_class)
    # model_name = "DFL-CNN"
    # weights_path = r"D:\TJU\GBDB\set113\models\test100\species\DFL-CNN\epoch_120_top1_75.000_checkpoint.pth.tar"
    # model = FineTuneResnet50(num_class=num_class, pretrained=False)
    # model_name = "PC"
    # weights_path = r"D:\TJU\GBDB\set113\models\test100\species\PC\epoch_0021_top1_76.00000_'checkpoint.pth.tar'"
    # model = BCNN_resnet50(num_class=num_class)
    # model_name = "B-CNN"
    # weights_path = r"D:\TJU\GBDB\set113\models\test100\species\B-CNN\epoch_0070_top1_69.000_'checkpoint.pth.tar'"
    # model = BCNN_resnet50(num_class=num_class)
    # model_name = "B-CNN"
    # weights_path = r"D:\TJU\GBDB\set113\models\test100\species\B-CNN\epoch_0070_top1_69.000_'checkpoint.pth.tar'"

    # device = torch.device(gpu)
    # model = model.cuda()
    # roc_points(model, model_name, weights_path)

    plot_function()
